package org.apache.hadoop.hbase.util;

package org.apache.hadoop.trigger;

import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.Enumeration;
import java.util.jar.JarEntry;
import java.util.jar.JarFile;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class Utils {

	/**
	 * copy jar file from local disk to HDFS
	 * @param jarPath: The jar path in local disk
	 * @param submitPath: the submit path in HDFS
	 * @throws IOException
	 */
  public static void submitTrigger(String jarPath, String submitPath)
			throws IOException {

		Configuration conf = new Configuration();
		FileSystem hdfs = FileSystem.get(conf);
		Path srcPath = new Path(jarPath);
		Path dstPath = new Path(submitPath);
		hdfs.copyFromLocalFile(srcPath, dstPath);

	}

	/**
	 * get jar file from HDFS to local disk
	 * @param localPath
	 * @param remotefile
	 * @throws IOException
	 */
	public static void getFromRemote(String localPath, String remotefile)
			throws IOException {
		Configuration conf = new Configuration();
		FileSystem hdfs = FileSystem.get(conf);
		hdfs.copyToLocalFile(new Path(remotefile), new Path(localPath));
	}

	//decompress the jar file in local disk
	public static synchronized void decompressInLocalDisk(String fileName,
			String outputPath) {

		if (!outputPath.endsWith(File.separator)) {
			outputPath += File.separator;
		}
		File dir = new File(outputPath);
		if (!dir.exists()) {
			dir.mkdirs();
		}
		JarFile jf = null;
		try {
			jf = new JarFile(fileName);
			for (Enumeration<JarEntry> e = jf.entries(); e.hasMoreElements();) {
				JarEntry je = (JarEntry) e.nextElement();
				String outFileName = outputPath + je.getName();
				File f = new File(outFileName);
				if (je.isDirectory()) {
					if (!f.exists()) {
						f.mkdirs();
					}
				} else {
					File pf = f.getParentFile();
					if (!pf.exists()) {
						pf.mkdirs();
					}
					InputStream in = jf.getInputStream(je);
					OutputStream out = new BufferedOutputStream(
							new FileOutputStream(f));
					byte[] buffer = new byte[2048];
					int nBytes = 0;
					while ((nBytes = in.read(buffer)) > 0) {
						out.write(buffer, 0, nBytes);
					}
					out.flush();
					out.close();
					in.close();
				}
			}
		} catch (Exception e) {
			System.out.println("decompress:" + fileName + "error --" + e.getMessage());
		} finally {
			if (jf != null) {
				try {
					jf.close();
				} catch (IOException e) {
					e.printStackTrace();
				}
			}
		}
	}
	
	public Object getInstance(String name) {
		Class cls;
		Object result = null;
		try {
			cls = Class.forName(name);
			result = cls.newInstance();
		} catch (ClassNotFoundException e) {
			e.printStackTrace();
		} catch (InstantiationException e) {
			e.printStackTrace();
		} catch (IllegalAccessException e) {
			e.printStackTrace();
		}
		
		return result;
	}
	
	public static void main(String args[]) {
		Utils.decompressInLocalDisk("d:\\hadoop-tools-1.1.2.jar", "d:\\hadoop-tools-1.1.2");
	}

}
